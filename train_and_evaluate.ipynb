{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7666ce-76f2-4d62-accd-f33663a602a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement install (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for install\u001b[0m\u001b[31m\n",
      "\u001b[0mUsing device: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets accelerate scikit-learn pandas install hf_transfer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BartForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch import nn\n",
    "\n",
    "# H100 Hardware Acceleration (PyTorch 2.4+)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Directory Setup\n",
    "BASE_DIR = \"/workspace\" \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"experiments\")\n",
    "RETRIEVAL_CACHE = os.path.join(DATA_DIR, \"retrieval_results.pkl\")\n",
    "VERIFIER_MODEL = \"facebook/bart-large\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d734365-0ad2-4e52-a27e-4f14142a4933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 9935 training claims.\n",
      "Retrieval results keys: dict_keys(['baseline', 'decomposed', 'repo'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "def load_json(path):\n",
    "    with open(os.path.join(DATA_DIR, path)) as f: return json.load(f)\n",
    "\n",
    "train_claims = load_json(\"train_claims_quantemp.json\")\n",
    "val_claims = load_json(\"val_claims_quantemp.json\")\n",
    "test_claims = load_json(\"test_claims_quantemp.json\")\n",
    "\n",
    "with open(RETRIEVAL_CACHE, \"rb\") as f:\n",
    "    retrieval_results = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_claims)} training claims.\")\n",
    "print(f\"Retrieval results keys: {retrieval_results.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4657de1-5296-432c-afad-cbfa0a50cf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5ea4facf364662bb4f2209ad892933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a9d11cdcf74174bb1397917bc4dbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69640a2754fa426481624a9725747b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feed0dd132a448d8b4a36113bc9e5fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7bf3e8298a4ea89037d6a5d87f5d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60d988156664ce983e547746037f863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0566c7a11ee341d093330e2baf8af9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1.80814278 0.57492463 1.41325196]\n"
     ]
    }
   ],
   "source": [
    "def normalize_label(label):\n",
    "    l = str(label).lower()\n",
    "    if any(x in l for x in [\"support\", \"true\", \"correct\"]): return 0\n",
    "    if any(x in l for x in [\"refute\", \"false\", \"pants\"]): return 1\n",
    "    return 2\n",
    "\n",
    "def create_examples(claims, evidence_dict, top_k=3):\n",
    "    examples = []\n",
    "    for idx, obj in enumerate(claims):\n",
    "        label = normalize_label(obj[\"label\"])\n",
    "        evs = evidence_dict.get(idx, [])\n",
    "        for ev in evs[:top_k]:\n",
    "            if len(ev.strip()) < 20: continue\n",
    "            examples.append({\"claim\": obj[\"claim\"], \"evidence\": ev[:1024], \"label\": label})\n",
    "    return examples\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(VERIFIER_MODEL)\n",
    "\n",
    "def process_ds(examples):\n",
    "    ds = Dataset.from_list(examples)\n",
    "    return ds.map(lambda x: tokenizer(\n",
    "        x[\"evidence\"], x[\"claim\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "    ), batched=True)\n",
    "\n",
    "train_ds = process_ds(create_examples(train_claims, retrieval_results[\"decomposed\"][\"train\"]))\n",
    "val_ds = process_ds(create_examples(val_claims, retrieval_results[\"decomposed\"][\"val\"]))\n",
    "\n",
    "# Compute weights for the loss function\n",
    "y_labels = [ex['label'] for ex in create_examples(train_claims, retrieval_results[\"decomposed\"][\"train\"])]\n",
    "weights = compute_class_weight(\"balanced\", classes=np.array([0,1,2]), y=y_labels)\n",
    "print(f\"Class weights: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226d1c83-ddbf-4905-977f-8ef1ca88c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Extract logits and labels\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    \n",
    "    # BART specific fix: extract first element if it's a tuple\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "        \n",
    "    # Get class predictions\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate global metrics\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    # Returns 4 arrays: (precisions, recalls, f1s, supports)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, preds, labels=[0, 1, 2], zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Create the dictionary of results\n",
    "    results = {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n",
    "    \n",
    "    # Add per-class details to the results\n",
    "    class_names = [\"support\", \"refute\", \"nei\"]\n",
    "    for i, name in enumerate(class_names):\n",
    "        results[f\"{name}_precision\"] = precision[i]\n",
    "        results[f\"{name}_recall\"] = recall[i]\n",
    "        results[f\"{name}_f1\"] = f1[i]\n",
    "        results[f\"{name}_support\"] = int(support[i])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5964d867-8f38-41f2-95ba-c8cbb2f9e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "W0116 18:20:47.546000 337 torch/fx/experimental/symbolic_shapes.py:6823] [8/5] _maybe_guard_rel() was called on non-relation expression Eq(s33, s6) | Eq(s6, 1)\n",
      "W0116 18:20:48.849000 337 torch/fx/experimental/symbolic_shapes.py:6823] [8/6] _maybe_guard_rel() was called on non-relation expression Eq(s33, s6) | Eq(s6, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3648' max='3648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3648/3648 1:07:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Support F1</th>\n",
       "      <th>Support Support</th>\n",
       "      <th>Refute F1</th>\n",
       "      <th>Refute Support</th>\n",
       "      <th>Nei F1</th>\n",
       "      <th>Nei Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.908592</td>\n",
       "      <td>0.605628</td>\n",
       "      <td>0.564822</td>\n",
       "      <td>0.503755</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.734357</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.456354</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.924532</td>\n",
       "      <td>0.638750</td>\n",
       "      <td>0.583768</td>\n",
       "      <td>0.537870</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.773115</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.440320</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.908773</td>\n",
       "      <td>0.632879</td>\n",
       "      <td>0.583829</td>\n",
       "      <td>0.527178</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.460868</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.938681</td>\n",
       "      <td>0.637532</td>\n",
       "      <td>0.582149</td>\n",
       "      <td>0.549989</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.772660</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.423799</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.944447</td>\n",
       "      <td>0.638529</td>\n",
       "      <td>0.585705</td>\n",
       "      <td>0.548945</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.769792</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.438377</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.988068</td>\n",
       "      <td>0.639304</td>\n",
       "      <td>0.591645</td>\n",
       "      <td>0.539968</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.768435</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.466531</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>1.050440</td>\n",
       "      <td>0.653595</td>\n",
       "      <td>0.596005</td>\n",
       "      <td>0.536433</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.786950</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.464632</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>1.085357</td>\n",
       "      <td>0.638197</td>\n",
       "      <td>0.584590</td>\n",
       "      <td>0.548479</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.772072</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.433220</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>1.079976</td>\n",
       "      <td>0.626565</td>\n",
       "      <td>0.580038</td>\n",
       "      <td>0.547166</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.757004</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.435944</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>1.140268</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.578189</td>\n",
       "      <td>0.541831</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.762463</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.430273</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>1.136725</td>\n",
       "      <td>0.639194</td>\n",
       "      <td>0.582065</td>\n",
       "      <td>0.540352</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.777554</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.428288</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>1.135309</td>\n",
       "      <td>0.630110</td>\n",
       "      <td>0.579527</td>\n",
       "      <td>0.538305</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.765632</td>\n",
       "      <td>5238</td>\n",
       "      <td>0.434643</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0116 18:21:06.713000 337 torch/fx/experimental/symbolic_shapes.py:6823] [8/7] _maybe_guard_rel() was called on non-relation expression Eq(s33, s6) | Eq(s6, 1)\n",
      "W0116 18:21:06.861000 337 torch/fx/experimental/symbolic_shapes.py:6823] [8/7] _maybe_guard_rel() was called on non-relation expression Eq(s21, s69) | Eq(s69, 1)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    BartForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    utils\n",
    ")\n",
    "\n",
    "# --- 1. THE PERMANENT SECURITY BYPASS ---\n",
    "# This forces the library to allow torch.load on PyTorch < 2.6\n",
    "import transformers.utils.import_utils as import_utils\n",
    "import_utils.check_torch_load_is_safe = lambda: True\n",
    "\n",
    "# --- 2. A40 HARDWARE ACCELERATION ---\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# --- 3. UPDATED METRICS (Per-Class + Global) ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Global metrics\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Per-class metrics (Support=0, Refute=1, NEI=2)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, preds, labels=[0, 1, 2], zero_division=0\n",
    "    )\n",
    "    \n",
    "    results = {\"accuracy\": acc, \"macro_f1\": macro_f1}\n",
    "    class_names = [\"support\", \"refute\", \"nei\"]\n",
    "    for i, name in enumerate(class_names):\n",
    "        results[f\"{name}_f1\"] = f1[i]\n",
    "        results[f\"{name}_support\"] = int(support[i])\n",
    "        \n",
    "    return results\n",
    "\n",
    "# --- 4. CUSTOM TRAINER WITH CLASS WEIGHTS ---\n",
    "class BartA40Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        # DEVICE and weights must be defined in your notebook scope\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32).to(DEVICE))\n",
    "        loss = loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 5. OPTIMIZED TRAINING ARGUMENTS ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"bart_a40_optimized\"),\n",
    "    \n",
    "    # Speed & Memory\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"aot_eager\",\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Fast Evaluation (Optimized for A40)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,                     # Increased to 500 for better speed\n",
    "    per_device_eval_batch_size=16,      # Higher batch size for eval\n",
    "    eval_accumulation_steps=50,         # Large chunks to CPU to avoid PCIe lag\n",
    "    \n",
    "    # Training Batch Config\n",
    "    per_device_train_batch_size=14,\n",
    "    gradient_accumulation_steps=2,      # Effective batch = 28\n",
    "    \n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# --- 6. INITIALIZE AND TRAIN ---\n",
    "model = BartForSequenceClassification.from_pretrained(\n",
    "    VERIFIER_MODEL, \n",
    "    num_labels=3,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "trainer = BartA40Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# RESUME FROM CHECKPOINT\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_bart_a40_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adcccf3e-d359-4236-988f-72bd739f8bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2495/2495 [00:57<00:00, 43.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Test Accuracy:    0.6369\n",
      "Test Macro-F1:   0.5889 (Treats all classes equal)\n",
      "Test Weighted-F1: 0.6495 (Accounts for class size)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model.eval()\n",
    "\n",
    "def test_eval(claims, evidence_dict):\n",
    "    preds, truths = [], []\n",
    "    for idx, obj in enumerate(tqdm(claims)):\n",
    "        label = normalize_label(obj[\"label\"])\n",
    "        evs = evidence_dict.get(idx, [])\n",
    "        \n",
    "        if not evs:\n",
    "            preds.append(2)  # Default to NEI if no evidence\n",
    "            truths.append(label)\n",
    "            continue\n",
    "        \n",
    "        # Inference\n",
    "        batch = tokenizer([e[:1024] for e in evs[:5]], [obj[\"claim\"]]*len(evs[:5]), \n",
    "                          padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                logits = model(**batch).logits\n",
    "            \n",
    "            # Average probabilities across all evidence snippets for this claim\n",
    "            avg_prob = torch.softmax(logits, dim=1).mean(dim=0)\n",
    "            preds.append(int(torch.argmax(avg_prob).cpu()))\n",
    "            truths.append(label)\n",
    "\n",
    "    # Calculate all three variations\n",
    "    m_f1 = f1_score(truths, preds, average=\"macro\")\n",
    "    w_f1 = f1_score(truths, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(truths, preds)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Test Accuracy:    {acc:.4f}\")\n",
    "    print(f\"Test Macro-F1:   {m_f1:.4f} (Treats all classes equal)\")\n",
    "    print(f\"Test Weighted-F1: {w_f1:.4f} (Accounts for class size)\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return m_f1, w_f1\n",
    "\n",
    "# Run the evaluation\n",
    "macro_f1, weighted_f1 = test_eval(test_claims, retrieval_results[\"decomposed\"][\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
