{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46276a3f25d64a079cbf275245106606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bed374cb5b694d4f87300d746cc421be",
              "IPY_MODEL_6dc2d821b55444f7af54fbeadd23001d",
              "IPY_MODEL_1175f18b21834b98a84f95805f8f7a79"
            ],
            "layout": "IPY_MODEL_af3372e831454556bf90e54a6c42fd48"
          }
        },
        "bed374cb5b694d4f87300d746cc421be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e5de05f7b940ceb3f5ff6c2c605787",
            "placeholder": "​",
            "style": "IPY_MODEL_aab1fa0989bb48cc98cd7d3aeb85de40",
            "value": "Map: 100%"
          }
        },
        "6dc2d821b55444f7af54fbeadd23001d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e179dcefabdf40ed8016c7d0622ced11",
            "max": 29178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0254d8e93a5e4fd1a4d5c121d7483bcd",
            "value": 29178
          }
        },
        "1175f18b21834b98a84f95805f8f7a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46b4ab830fee4c398acced3db6abee1b",
            "placeholder": "​",
            "style": "IPY_MODEL_535857e962394da696030fca09d8f809",
            "value": " 29178/29178 [00:12&lt;00:00, 3368.84 examples/s]"
          }
        },
        "af3372e831454556bf90e54a6c42fd48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e5de05f7b940ceb3f5ff6c2c605787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab1fa0989bb48cc98cd7d3aeb85de40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e179dcefabdf40ed8016c7d0622ced11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0254d8e93a5e4fd1a4d5c121d7483bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46b4ab830fee4c398acced3db6abee1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "535857e962394da696030fca09d8f809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e75edb7c5f249d1b2fa08b66b856a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02ef593206ed49d39c7b7c3383e54d27",
              "IPY_MODEL_17d5ebfd188a44dbad8b74510cfd303a",
              "IPY_MODEL_1553b4a5361e497c98d7d3595d7bd2f5"
            ],
            "layout": "IPY_MODEL_0601c10e2e1a43d49989ec0909294a13"
          }
        },
        "02ef593206ed49d39c7b7c3383e54d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519ede3581b449eeb6fee859f9e78329",
            "placeholder": "​",
            "style": "IPY_MODEL_4ee565e372da46b69fd7692f93e2b1c0",
            "value": "Map: 100%"
          }
        },
        "17d5ebfd188a44dbad8b74510cfd303a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e2b03d94c234d5895a771fa443bf998",
            "max": 9027,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54fc86f7b9024c989fdb57b798061289",
            "value": 9027
          }
        },
        "1553b4a5361e497c98d7d3595d7bd2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66f19a8c720246508a81d4d481d7d7ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f26f8fc792c44d0dbae4eafe47fd3f32",
            "value": " 9027/9027 [00:02&lt;00:00, 3586.18 examples/s]"
          }
        },
        "0601c10e2e1a43d49989ec0909294a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519ede3581b449eeb6fee859f9e78329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee565e372da46b69fd7692f93e2b1c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e2b03d94c234d5895a771fa443bf998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fc86f7b9024c989fdb57b798061289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66f19a8c720246508a81d4d481d7d7ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26f8fc792c44d0dbae4eafe47fd3f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== 0. SETUP & ENVIRONMENT =====\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 0: Environment Setup\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create experiment directory\n",
        "import os\n",
        "EXPERIMENT_DIR = \"/content/drive/MyDrive/NLP\"\n",
        "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
        "print(f\"\\nExperiment directory: {EXPERIMENT_DIR}\")\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nInstalling dependencies...\")\n",
        "!pip install -q transformers datasets torch scikit-learn pandas rank-bm25 sentencepiece\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"WARNING: Running on CPU - training will be slow!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq5AHXKtvdUg",
        "outputId": "1d1dc71c-5473-4b60-d61b-687b694e9340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 0: Environment Setup\n",
            "================================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Experiment directory: /content/drive/MyDrive/NLP\n",
            "\n",
            "Installing dependencies...\n",
            "\n",
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 1. DOWNLOAD DATA =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 1: Downloading Datasets\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Clone QuanTemp repo\n",
        "if not os.path.exists(\"/content/QuanTemp\"):\n",
        "    print(\"\\nCloning QuanTemp repository...\")\n",
        "    !git clone https://github.com/factiverse/QuanTemp.git\n",
        "    print(\"✓ QuanTemp cloned\")\n",
        "else:\n",
        "    print(\"✓ QuanTemp already exists\")\n",
        "\n",
        "# Clone FactIR repo\n",
        "if not os.path.exists(\"/content/factIR\"):\n",
        "    print(\"\\nCloning FactIR repository...\")\n",
        "    !git clone https://github.com/factiverse/factIR.git\n",
        "    print(\"✓ FactIR cloned\")\n",
        "else:\n",
        "    print(\"✓ FactIR already exists\")\n",
        "\n",
        "# Download QuanTemp data if needed\n",
        "os.chdir(\"/content/QuanTemp\")\n",
        "if not os.path.exists(\"data/bm25_scored_evidence/bm25_top_100_claimdecomp.json\"):\n",
        "    print(\"\\nDownloading QuanTemp pre-processed data...\")\n",
        "    !pip install -q gdown\n",
        "    !gdown --folder 1GYzSK0oU2MiaKbyBO3hE8kO4gdmxDjCv -O /content/QuanTemp/data --remaining-ok\n",
        "\n",
        "    # Unzip BM25 evidence if zipped\n",
        "    if os.path.exists(\"data/bm25_scored_evidence/bm25_top_100_claimdecomp.json.zip\"):\n",
        "        !unzip -q data/bm25_scored_evidence/bm25_top_100_claimdecomp.json.zip -d data/bm25_scored_evidence/\n",
        "    print(\"✓ QuanTemp data downloaded\")\n",
        "else:\n",
        "    print(\"✓ QuanTemp data already exists\")\n",
        "\n",
        "# Verify files\n",
        "print(\"\\nVerifying required files...\")\n",
        "required_files = [\n",
        "    \"/content/QuanTemp/data/raw_data/train_claims_quantemp.json\",\n",
        "    \"/content/QuanTemp/data/raw_data/val_claims_quantemp.json\",\n",
        "    \"/content/QuanTemp/data/raw_data/test_claims_quantemp.json\",\n",
        "    \"/content/QuanTemp/data/bm25_scored_evidence/bm25_top_100_claimdecomp.json\",\n",
        "    \"/content/factIR/evidence.csv\"\n",
        "]\n",
        "\n",
        "all_exist = True\n",
        "for f in required_files:\n",
        "    exists = os.path.exists(f)\n",
        "    status = \"✓\" if exists else \"✗ MISSING\"\n",
        "    print(f\"{status} {f}\")\n",
        "    if not exists:\n",
        "        all_exist = False\n",
        "\n",
        "if not all_exist:\n",
        "    print(\"\\n⚠️  Some files are missing. Please ensure:\")\n",
        "    print(\"   1. QuanTemp data is downloaded\")\n",
        "    print(\"   2. FactIR evidence.csv exists\")\n",
        "    print(\"   3. You've uploaded the QuanTemp evidence corpus JSON if needed\")\n",
        "    raise FileNotFoundError(\"Required data files missing\")\n",
        "\n",
        "print(\"\\n✓ All required files present\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqIKYYRqyD1Y",
        "outputId": "87bfe37a-d405-4112-e55e-ad9dae975b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 1: Downloading Datasets\n",
            "================================================================================\n",
            "✓ QuanTemp already exists\n",
            "✓ FactIR already exists\n",
            "✓ QuanTemp data already exists\n",
            "\n",
            "Verifying required files...\n",
            "✓ /content/QuanTemp/data/raw_data/train_claims_quantemp.json\n",
            "✓ /content/QuanTemp/data/raw_data/val_claims_quantemp.json\n",
            "✓ /content/QuanTemp/data/raw_data/test_claims_quantemp.json\n",
            "✓ /content/QuanTemp/data/bm25_scored_evidence/bm25_top_100_claimdecomp.json\n",
            "✓ /content/factIR/evidence.csv\n",
            "\n",
            "✓ All required files present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 2. LOAD FREE LLM FOR DECOMPOSITION =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 2: Loading Free LLM for Decomposition\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Use FLAN-T5-base (free, local model)\n",
        "LLM_MODEL_NAME = \"google/flan-t5-base\"\n",
        "print(f\"\\nLoading {LLM_MODEL_NAME}...\")\n",
        "\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
        "llm_model = llm_model.to(device)\n",
        "llm_model.eval()\n",
        "\n",
        "print(f\"✓ LLM loaded on {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgSga05ByeeF",
        "outputId": "272f2aa9-4541-4077-b351-c36af4c844ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 2: Loading Free LLM for Decomposition\n",
            "================================================================================\n",
            "\n",
            "Loading google/flan-t5-base...\n",
            "✓ LLM loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # ===== 3. DECOMPOSITION METHODS =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 3: Defining Decomposition Methods\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def clean_and_deduplicate(subqueries: List[str], min_tokens: int = 4) -> List[str]:\n",
        "    \"\"\"Clean, deduplicate, and filter subqueries\"\"\"\n",
        "    cleaned = []\n",
        "    seen = set()\n",
        "\n",
        "    for sq in subqueries:\n",
        "        sq = sq.strip()\n",
        "        if len(sq) == 0:\n",
        "            continue\n",
        "\n",
        "        # Remove numbering like \"1.\", \"2.\", etc.\n",
        "        sq = re.sub(r'^\\d+[\\.\\)]\\s*', '', sq)\n",
        "        sq = sq.strip()\n",
        "\n",
        "        # Check minimum tokens\n",
        "        if len(sq.split()) < min_tokens:\n",
        "            continue\n",
        "\n",
        "        # Deduplicate (case-insensitive)\n",
        "        sq_lower = sq.lower()\n",
        "        if sq_lower not in seen:\n",
        "            seen.add(sq_lower)\n",
        "            cleaned.append(sq)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def decompose_with_llm(claim: str, prompt_template: str, max_subqueries: int = 4) -> List[str]:\n",
        "    \"\"\"Generate decomposition using LLM\"\"\"\n",
        "    prompt = prompt_template.format(claim=claim)\n",
        "\n",
        "    inputs = llm_tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=256,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Split by newlines or numbered items\n",
        "    subqueries = re.split(r'\\n|(?=\\d+[\\.\\)])', response)\n",
        "    subqueries = clean_and_deduplicate(subqueries)\n",
        "\n",
        "    return subqueries[:max_subqueries]\n",
        "\n",
        "# Define 3 decomposition strategies\n",
        "# DECOMPOSITION_METHODS = {\n",
        "#     \"baseline\": {\n",
        "#         \"name\": \"Baseline (No Decomposition)\",\n",
        "#         \"func\": lambda claim: [claim]  # Just return original claim\n",
        "#     },\n",
        "\n",
        "#     \"llm_numeric\": {\n",
        "#         \"name\": \"LLM: Numeric & Temporal Focus\",\n",
        "#         \"func\": lambda claim: decompose_with_llm(\n",
        "#             claim,\n",
        "#             \"Split this fact-checking claim into 2-4 atomic subclaims. Focus on extracting specific numbers, dates, and temporal constraints separately. Keep each subclaim concise.\\n\\nClaim: {claim}\\n\\nSubclaims:\"\n",
        "#         )\n",
        "#     },\n",
        "\n",
        "#     \"llm_constraints\": {\n",
        "#         \"name\": \"LLM: Constraint Extraction\",\n",
        "#         \"func\": lambda claim: decompose_with_llm(\n",
        "#             claim,\n",
        "#             \"Extract 2-4 key factual constraints from this claim as separate queries. Separate numeric constraints from categorical/entity constraints.\\n\\nClaim: {claim}\\n\\nConstraints:\"\n",
        "#         )\n",
        "#     },\n",
        "\n",
        "#     \"llm_keywords\": {\n",
        "#         \"name\": \"LLM: Keyword Queries\",\n",
        "#         \"func\": lambda claim: decompose_with_llm(\n",
        "#             claim,\n",
        "#             \"Rewrite this claim into 2-4 short keyword-based search queries to find relevant evidence. Focus on core facts that can be verified.\\n\\nClaim: {claim}\\n\\nQueries:\"\n",
        "#         )\n",
        "#     }\n",
        "# }\n",
        "\n",
        "print(\"Decomposition methods defined:\")\n",
        "for key, method in DECOMPOSITION_METHODS.items():\n",
        "    print(f\"  • {key}: {method['name']}\")\n",
        "\n",
        "# Test decomposition\n",
        "print(\"\\nTesting decomposition on sample claim...\")\n",
        "\n",
        "test_claim = \"\\\"When you throw 23 million people off of health insurance -- people with cancer, people with heart disease, people with diabetes -- thousands of people will die. \\u2026 This is study after study making this point.\"\n",
        "for key, method in DECOMPOSITION_METHODS.items():\n",
        "    result = method[\"func\"](test_claim)\n",
        "    print(f\"\\n{method['name']}:\")\n",
        "    for i, sq in enumerate(result, 1):\n",
        "        print(f\"  {i}. {sq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loq4pLtE3tHV",
        "outputId": "df4a8cf1-05ff-412c-b565-beb03ca3a8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 3: Defining Decomposition Methods\n",
            "================================================================================\n",
            "Decomposition methods defined:\n",
            "  • baseline: Baseline (No Decomposition)\n",
            "  • llm_numeric: LLM v2: Numeric & Temporal (Anchored)\n",
            "  • llm_constraints: LLM v2: Entity + Constraint Queries\n",
            "  • llm_keywords: LLM v2: High-Precision Keyword Queries\n",
            "\n",
            "Testing decomposition on sample claim...\n",
            "\n",
            "Baseline (No Decomposition):\n",
            "  1. \"When you throw 23 million people off of health insurance -- people with cancer, people with heart disease, people with diabetes -- thousands of people will die. … This is study after study making this point.\n",
            "\n",
            "LLM v2: Numeric & Temporal (Anchored):\n",
            "\n",
            "LLM v2: Entity + Constraint Queries:\n",
            "  1. WHat is the number of people with cancer?\n",
            "\n",
            "LLM v2: High-Precision Keyword Queries:\n",
            "  1. BM25: When you throw 23 million people off of health insurance -- people with cancer, people with heart disease, people with diabetes -- thousands of people will die. ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Improved Decomposition Strategies (BM25-Anchored) =====\n",
        "DECOMPOSITION_METHODS = {\n",
        "\n",
        "    \"baseline\": {\n",
        "        \"name\": \"Baseline (No Decomposition)\",\n",
        "        \"func\": lambda claim: [claim]\n",
        "    },\n",
        "\n",
        "    \"llm_numeric\": {\n",
        "        \"name\": \"LLM v2: Numeric & Temporal (Anchored)\",\n",
        "        \"func\": lambda claim: decompose_with_llm(\n",
        "            claim,\n",
        "            \"\"\"\n",
        "You are generating BM25 search queries. DO NOT paraphrase the claim.\n",
        "\n",
        "Rules:\n",
        "- Reuse exact words and phrases from the claim (copy substrings).\n",
        "- Keep all numbers, dates, percentages, and currencies EXACTLY as written.\n",
        "- Each query must contain at least one number or temporal expression.\n",
        "- Output 2–4 queries, one per line.\n",
        "- Each query must be short (6–12 tokens).\n",
        "- Avoid generic terms like \"report\", \"study\", \"data\".\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Queries:\n",
        "\"\"\".strip()\n",
        "        )\n",
        "    },\n",
        "\n",
        "    \"llm_constraints\": {\n",
        "        \"name\": \"LLM v2: Entity + Constraint Queries\",\n",
        "        \"func\": lambda claim: decompose_with_llm(\n",
        "            claim,\n",
        "            \"\"\"\n",
        "Generate 2–4 precise BM25 search queries.\n",
        "\n",
        "Rules:\n",
        "- Each query must include:\n",
        "  (1) a named entity copied exactly from the claim AND\n",
        "  (2) a numeric or temporal constraint copied exactly from the claim.\n",
        "- Do NOT paraphrase entities or numbers.\n",
        "- Keep queries concise (6–12 tokens).\n",
        "- If no explicit number exists, use exact quantifier phrases (\"most\", \"all\", \"only\").\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Queries:\n",
        "\"\"\".strip()\n",
        "        )\n",
        "    },\n",
        "\n",
        "    \"llm_keywords\": {\n",
        "        \"name\": \"LLM v2: High-Precision Keyword Queries\",\n",
        "        \"func\": lambda claim: decompose_with_llm(\n",
        "            claim,\n",
        "            \"\"\"\n",
        "Rewrite the claim into 2–4 HIGH-PRECISION keyword queries for BM25.\n",
        "\n",
        "Rules:\n",
        "- Copy all named entities exactly as they appear.\n",
        "- Copy all numbers, dates, and units exactly.\n",
        "- Add at most ONE extra descriptive token if necessary.\n",
        "- Each query must be <= 10 tokens.\n",
        "- Prefer rare and specific words over generic ones.\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Queries:\n",
        "\"\"\".strip()\n",
        "        )\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "3REfOLWs_Cfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 4. FACTIR EVALUATION =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 4: FactIR Ranking Evaluation\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Load FactIR data\n",
        "print(\"\\nLoading FactIR evidence.csv...\")\n",
        "factir_df = pd.read_csv(\"/content/factIR/evidence.csv\")\n",
        "print(f\"✓ Loaded {len(factir_df)} rows with {factir_df['claim'].nunique()} unique claims\")\n",
        "\n",
        "# Simple tokenizer\n",
        "def tokenize(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    return [t for t in text.split() if t]\n",
        "\n",
        "# Ranking metrics\n",
        "def recall_at_k(rels_sorted, k):\n",
        "    return 1.0 if any(rels_sorted[:k]) else 0.0\n",
        "\n",
        "def mrr(rels_sorted):\n",
        "    for i, r in enumerate(rels_sorted, start=1):\n",
        "        if r == 1:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "# Cache for decompositions\n",
        "FACTIR_DECOMP_CACHE = os.path.join(EXPERIMENT_DIR, \"factir_decompositions.pkl\")\n",
        "\n",
        "if os.path.exists(FACTIR_DECOMP_CACHE):\n",
        "    print(f\"\\nLoading cached decompositions from {FACTIR_DECOMP_CACHE}\")\n",
        "    with open(FACTIR_DECOMP_CACHE, \"rb\") as f:\n",
        "        decomp_cache = pickle.load(f)\n",
        "else:\n",
        "    print(\"\\nGenerating decompositions for FactIR claims (this may take time)...\")\n",
        "    decomp_cache = {}\n",
        "\n",
        "    unique_claims = factir_df['claim'].unique()\n",
        "\n",
        "    for method_key in tqdm(DECOMPOSITION_METHODS.keys(), desc=\"Methods\"):\n",
        "        decomp_cache[method_key] = {}\n",
        "        method_func = DECOMPOSITION_METHODS[method_key][\"func\"]\n",
        "\n",
        "        for claim in tqdm(unique_claims, desc=f\"  {method_key}\", leave=False):\n",
        "            decomp_cache[method_key][claim] = method_func(claim)\n",
        "\n",
        "    # Save cache\n",
        "    with open(FACTIR_DECOMP_CACHE, \"wb\") as f:\n",
        "        pickle.dump(decomp_cache, f)\n",
        "    print(f\"✓ Saved decomposition cache to {FACTIR_DECOMP_CACHE}\")\n",
        "\n",
        "# Evaluate each method\n",
        "print(\"\\nEvaluating decomposition methods on FactIR...\")\n",
        "\n",
        "factir_results = []\n",
        "skipped_claims = 0\n",
        "for method_key, method_info in DECOMPOSITION_METHODS.items():\n",
        "    print(f\"\\nEvaluating: {method_info['name']}\")\n",
        "\n",
        "    method_metrics = []\n",
        "\n",
        "    for claim, group in tqdm(factir_df.groupby(\"claim\"), desc=\"  Claims\", leave=False):\n",
        "\n",
        "        snippets = group[\"snippet\"].fillna(\"\").tolist()\n",
        "        rels = group[\"relevance\"].astype(int).tolist()\n",
        "\n",
        "        # Get subqueries for this claim\n",
        "        subqueries = decomp_cache[method_key][claim]\n",
        "\n",
        "        # Tokenize documents\n",
        "        tokenized_docs = [tokenize(s) for s in snippets]\n",
        "\n",
        "        if method_key == \"baseline\":\n",
        "            # Baseline: single query\n",
        "            bm25 = BM25Okapi(tokenized_docs)\n",
        "            query_tokens = tokenize(claim)\n",
        "            scores = bm25.get_scores(query_tokens)\n",
        "        else:\n",
        "            bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "            # Baseline score\n",
        "            base_scores = bm25.get_scores(tokenize(claim))\n",
        "\n",
        "            # Decomposition score: mean over all subqueries\n",
        "            if len(subqueries) > 0:\n",
        "                decomp_scores = np.mean(\n",
        "                    [bm25.get_scores(tokenize(q)) for q in subqueries],\n",
        "                    axis=0\n",
        "                )\n",
        "            else:\n",
        "                decomp_scores = np.zeros(len(snippets))\n",
        "\n",
        "            # Option B: mixture of baseline + decomposition\n",
        "            alpha = 0.6\n",
        "            scores = alpha * base_scores + (1 - alpha) * decomp_scores\n",
        "\n",
        "        # Rank by scores\n",
        "        order = np.argsort(scores)[::-1]\n",
        "        rels_sorted = [rels[i] for i in order]\n",
        "\n",
        "        method_metrics.append({\n",
        "            \"claim\": claim,\n",
        "            \"method\": method_key,\n",
        "            \"hit@5\": recall_at_k(rels_sorted, 5),\n",
        "            \"hit@10\": recall_at_k(rels_sorted, 10),\n",
        "            \"mrr\": mrr(rels_sorted),\n",
        "            \"num_subqueries\": len(subqueries),\n",
        "            \"num_candidates\": len(snippets),\n",
        "            \"num_relevant\": int(sum(rels))\n",
        "        })\n",
        "\n",
        "    # Compute averages\n",
        "    metrics_df = pd.DataFrame(method_metrics)\n",
        "    avg_hit5 = metrics_df[\"hit@5\"].mean()\n",
        "    avg_hit10 = metrics_df[\"hit@10\"].mean()\n",
        "    avg_mrr = metrics_df[\"mrr\"].mean()\n",
        "\n",
        "    print(f\"  Hit@5:  {avg_hit5:.4f}\")\n",
        "    print(f\"  Hit@10: {avg_hit10:.4f}\")\n",
        "    print(f\"  MRR:    {avg_mrr:.4f}\")\n",
        "\n",
        "    factir_results.append({\n",
        "        \"method\": method_key,\n",
        "        \"method_name\": method_info[\"name\"],\n",
        "        \"hit@5\": avg_hit5,\n",
        "        \"hit@10\": avg_hit10,\n",
        "        \"mrr\": avg_mrr,\n",
        "        \"per_claim_results\": method_metrics\n",
        "    })\n",
        "\n",
        "# Save FactIR results\n",
        "factir_summary_df = pd.DataFrame([\n",
        "    {\n",
        "        \"method\": r[\"method\"],\n",
        "        \"method_name\": r[\"method_name\"],\n",
        "        \"hit@5\": r[\"hit@5\"],\n",
        "        \"hit@10\": r[\"hit@10\"],\n",
        "        \"mrr\": r[\"mrr\"]\n",
        "    }\n",
        "    for r in factir_results\n",
        "])\n",
        "\n",
        "factir_summary_path = os.path.join(EXPERIMENT_DIR, \"factir_ranking_results.csv\")\n",
        "factir_summary_df.to_csv(factir_summary_path, index=False)\n",
        "print(f\"\\n✓ Saved FactIR summary to {factir_summary_path}\")\n",
        "\n",
        "# Select best decomposition method\n",
        "best_method = factir_summary_df.loc[factir_summary_df[\"mrr\"].idxmax()]\n",
        "BEST_DECOMP_KEY = best_method[\"method\"]\n",
        "print(f\"\\n🏆 BEST DECOMPOSITION METHOD: {best_method['method_name']}\")\n",
        "print(f\"   MRR: {best_method['mrr']:.4f}\")\n",
        "\n",
        "# Save detailed per-claim results\n",
        "for result in factir_results:\n",
        "    method_key = result[\"method\"]\n",
        "    per_claim_df = pd.DataFrame(result[\"per_claim_results\"])\n",
        "    per_claim_path = os.path.join(EXPERIMENT_DIR, f\"factir_{method_key}_perclaim.csv\")\n",
        "    per_claim_df.to_csv(per_claim_path, index=False)\n",
        "    print(f\"   Saved {method_key} per-claim results to {per_claim_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw4yW0DL4GGS",
        "outputId": "eec99f95-7f91-4da8-d427-b3558eb74fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 4: FactIR Ranking Evaluation\n",
            "================================================================================\n",
            "\n",
            "Loading FactIR evidence.csv...\n",
            "✓ Loaded 1412 rows with 100 unique claims\n",
            "\n",
            "Loading cached decompositions from /content/drive/MyDrive/NLP/factir_decompositions.pkl\n",
            "\n",
            "Evaluating decomposition methods on FactIR...\n",
            "\n",
            "Evaluating: Baseline (No Decomposition)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hit@5:  0.8100\n",
            "  Hit@10: 0.8300\n",
            "  MRR:    0.6739\n",
            "\n",
            "Evaluating: LLM v2: Numeric & Temporal (Anchored)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hit@5:  0.8000\n",
            "  Hit@10: 0.8400\n",
            "  MRR:    0.6772\n",
            "\n",
            "Evaluating: LLM v2: Entity + Constraint Queries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hit@5:  0.8200\n",
            "  Hit@10: 0.8400\n",
            "  MRR:    0.6523\n",
            "\n",
            "Evaluating: LLM v2: High-Precision Keyword Queries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hit@5:  0.8100\n",
            "  Hit@10: 0.8300\n",
            "  MRR:    0.6710\n",
            "\n",
            "✓ Saved FactIR summary to /content/drive/MyDrive/NLP/factir_ranking_results.csv\n",
            "\n",
            "🏆 BEST DECOMPOSITION METHOD: LLM v2: Numeric & Temporal (Anchored)\n",
            "   MRR: 0.6772\n",
            "   Saved baseline per-claim results to /content/drive/MyDrive/NLP/factir_baseline_perclaim.csv\n",
            "   Saved llm_numeric per-claim results to /content/drive/MyDrive/NLP/factir_llm_numeric_perclaim.csv\n",
            "   Saved llm_constraints per-claim results to /content/drive/MyDrive/NLP/factir_llm_constraints_perclaim.csv\n",
            "   Saved llm_keywords per-claim results to /content/drive/MyDrive/NLP/factir_llm_keywords_perclaim.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 5. QUANTEMP RETRIEVAL (LOAD ONLY) =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 5: QuanTemp Retrieval (Load-only, no recompute)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os, json, pickle\n",
        "\n",
        "# Load QuanTemp claims\n",
        "print(\"\\nLoading QuanTemp claims...\")\n",
        "with open(\"/content/QuanTemp/data/raw_data/train_claims_quantemp.json\") as f:\n",
        "    train_claims = json.load(f)\n",
        "with open(\"/content/QuanTemp/data/raw_data/val_claims_quantemp.json\") as f:\n",
        "    val_claims = json.load(f)\n",
        "with open(\"/content/QuanTemp/data/raw_data/test_claims_quantemp.json\") as f:\n",
        "    test_claims = json.load(f)\n",
        "\n",
        "print(f\"✓ Train: {len(train_claims)} claims\")\n",
        "print(f\"✓ Val:   {len(val_claims)} claims\")\n",
        "print(f\"✓ Test:  {len(test_claims)} claims\")\n",
        "\n",
        "# Load repo evidence dict (optional; keep if later you want R2 comparisons)\n",
        "print(\"\\nLoading repo bm25_top_100_claimdecomp.json (optional R2)...\")\n",
        "with open(\"/content/QuanTemp/data/bm25_scored_evidence/bm25_top_100_claimdecomp.json\") as f:\n",
        "    repo_bm25_data = json.load(f)\n",
        "repo_evidence_dict = {item[\"query_id\"]: item[\"docs\"] for item in repo_bm25_data}\n",
        "print(f\"✓ Repo evidence entries: {len(repo_evidence_dict)} query_ids\")\n",
        "\n",
        "# Load your already computed retrieval results (baseline + decomposed)\n",
        "RETRIEVAL_CACHE = os.path.join(EXPERIMENT_DIR, \"retrieval_results.pkl\")\n",
        "print(f\"\\nLoading retrieval cache: {RETRIEVAL_CACHE}\")\n",
        "with open(RETRIEVAL_CACHE, \"rb\") as f:\n",
        "    retrieval_results = pickle.load(f)\n",
        "\n",
        "print(\"✓ Loaded retrieval_results keys:\", list(retrieval_results.keys()))\n",
        "print(\"✓ baseline sizes:\", {k: len(v) for k, v in retrieval_results[\"baseline\"].items()})\n",
        "print(\"✓ decomposed sizes:\", {k: len(v) for k, v in retrieval_results[\"decomposed\"].items()})\n",
        "\n",
        "# Quick evidence coverage stats (no filtering)\n",
        "def coverage(d, n):\n",
        "    return sum(len(d.get(i, [])) > 0 for i in range(n)), n\n",
        "\n",
        "for split_name, claims in [(\"train\", train_claims), (\"val\", val_claims), (\"test\", test_claims)]:\n",
        "    n = len(claims)\n",
        "    c0, _ = coverage(retrieval_results[\"baseline\"][split_name], n)\n",
        "    c1, _ = coverage(retrieval_results[\"decomposed\"][split_name], n)\n",
        "    print(f\"\\nCoverage {split_name.upper()}:\")\n",
        "    print(f\"  R0 baseline:   {c0}/{n}\")\n",
        "    print(f\"  R1 decomposed: {c1}/{n}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0SpQLyDT9-k",
        "outputId": "d2d4bd0e-67de-4396-f9a9-4a70338da858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 5: QuanTemp Retrieval (Load-only, no recompute)\n",
            "================================================================================\n",
            "\n",
            "Loading QuanTemp claims...\n",
            "✓ Train: 9935 claims\n",
            "✓ Val:   3084 claims\n",
            "✓ Test:  2495 claims\n",
            "\n",
            "Loading repo bm25_top_100_claimdecomp.json (optional R2)...\n",
            "✓ Repo evidence entries: 2495 query_ids\n",
            "\n",
            "Loading retrieval cache: /content/drive/MyDrive/NLP/retrieval_results.pkl\n",
            "✓ Loaded retrieval_results keys: ['baseline', 'decomposed', 'repo']\n",
            "✓ baseline sizes: {'train': 9935, 'val': 3084, 'test': 2495}\n",
            "✓ decomposed sizes: {'train': 9935, 'val': 3084, 'test': 2495}\n",
            "\n",
            "Coverage TRAIN:\n",
            "  R0 baseline:   9935/9935\n",
            "  R1 decomposed: 9726/9935\n",
            "\n",
            "Coverage VAL:\n",
            "  R0 baseline:   3084/3084\n",
            "  R1 decomposed: 3009/3084\n",
            "\n",
            "Coverage TEST:\n",
            "  R0 baseline:   2495/2495\n",
            "  R1 decomposed: 2445/2495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 6. BUILD TRAINING PAIRS (DECOMPOSED ONLY, TOP-3, NO FILTER) =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 6: Building Training Pairs (R1 decomposed only, top-3, no filtering)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def normalize_label(label):\n",
        "    label_lower = label.lower()\n",
        "    if \"support\" in label_lower or \"true\" in label_lower or \"correct\" in label_lower:\n",
        "        return 0\n",
        "    elif \"refute\" in label_lower or \"false\" in label_lower or \"pants\" in label_lower:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "def create_training_examples_all(claims_list, evidence_dict, top_k=3, max_evidence_chars=512):\n",
        "    \"\"\"\n",
        "    Build (evidence, claim, label) pairs for ALL claim indices [0..len(claims_list)-1].\n",
        "    Uses top_k evidence snippets per claim.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for idx, claim_obj in enumerate(tqdm(claims_list, desc=\"Creating pairs\")):\n",
        "        claim_text = claim_obj[\"claim\"]\n",
        "        label = normalize_label(claim_obj[\"label\"])\n",
        "\n",
        "        evidences = evidence_dict.get(idx, [])\n",
        "        if not evidences:\n",
        "            continue\n",
        "\n",
        "        for ev in evidences[:top_k]:\n",
        "            ev = ev.strip()\n",
        "            if len(ev) < 20:\n",
        "                continue\n",
        "            examples.append({\n",
        "                \"claim\": claim_text,\n",
        "                \"evidence\": ev[:max_evidence_chars],\n",
        "                \"label\": label\n",
        "            })\n",
        "    return examples\n",
        "\n",
        "print(\"\\nBuilding TrainSet-B (R1 decomposed retrieval)...\")\n",
        "trainset_b = create_training_examples_all(\n",
        "    train_claims,\n",
        "    retrieval_results[\"decomposed\"][\"train\"],\n",
        "    top_k=3\n",
        ")\n",
        "valset_b = create_training_examples_all(\n",
        "    val_claims,\n",
        "    retrieval_results[\"decomposed\"][\"val\"],\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "print(f\"✓ TrainSet-B: {len(trainset_b)} examples\")\n",
        "print(f\"✓ ValSet-B:   {len(valset_b)} examples\")\n",
        "\n",
        "# (Optional) show label distribution\n",
        "from collections import Counter\n",
        "def print_label_dist(examples, name):\n",
        "    labels = [ex[\"label\"] for ex in examples]\n",
        "    dist = Counter(labels)\n",
        "    total = len(labels) if len(labels) else 1\n",
        "    print(f\"\\n{name} Label Distribution:\")\n",
        "    print(f\"  SUPPORTS (0): {dist[0]:5d} ({dist[0]/total*100:5.1f}%)\")\n",
        "    print(f\"  REFUTES  (1): {dist[1]:5d} ({dist[1]/total*100:5.1f}%)\")\n",
        "    print(f\"  NEI      (2): {dist[2]:5d} ({dist[2]/total*100:5.1f}%)\")\n",
        "\n",
        "print_label_dist(trainset_b, \"TrainSet-B (R1)\")\n",
        "print_label_dist(valset_b,   \"ValSet-B (R1)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-dA96vdopif",
        "outputId": "4ddf7f2e-ca27-4aef-e989-d94810638a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 6: Building Training Pairs (R1 decomposed only, top-3, no filtering)\n",
            "================================================================================\n",
            "\n",
            "Building TrainSet-B (R1 decomposed retrieval)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating pairs: 100%|██████████| 9935/9935 [00:00<00:00, 46222.03it/s]\n",
            "Creating pairs: 100%|██████████| 3084/3084 [00:00<00:00, 201687.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ TrainSet-B: 29178 examples\n",
            "✓ ValSet-B:   9027 examples\n",
            "\n",
            "TrainSet-B (R1) Label Distribution:\n",
            "  SUPPORTS (0):  5379 ( 18.4%)\n",
            "  REFUTES  (1): 16917 ( 58.0%)\n",
            "  NEI      (2):  6882 ( 23.6%)\n",
            "\n",
            "ValSet-B (R1) Label Distribution:\n",
            "  SUPPORTS (0):  1803 ( 20.0%)\n",
            "  REFUTES  (1):  5238 ( 58.0%)\n",
            "  NEI      (2):  1986 ( 22.0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 7. FINE-TUNE TWO VERIFIERS (A=R0, B=R1) =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 7: Fine-tuning Verifiers (A=baseline, B=decomposed)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch import nn\n",
        "\n",
        "VERIFIER_MODEL = \"roberta-base\"\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "    per_class = f1_score(labels, preds, average=None, zero_division=0)  # [supports, refutes, nei]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"supports_f1\": per_class[0],\n",
        "        \"refutes_f1\": per_class[1],\n",
        "        \"nei_f1\": per_class[2],\n",
        "    }\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        weight = torch.tensor(self.class_weights, dtype=torch.float32).to(logits.device)\n",
        "        loss = nn.CrossEntropyLoss(weight=weight)(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def tokenize_function(batch, tokenizer):\n",
        "    return tokenizer(\n",
        "        batch[\"evidence\"],\n",
        "        batch[\"claim\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "def build_hf_dataset(examples, tokenizer):\n",
        "    ds = Dataset.from_dict({\n",
        "        \"evidence\": [ex[\"evidence\"] for ex in examples],\n",
        "        \"claim\":    [ex[\"claim\"] for ex in examples],\n",
        "        \"label\":    [ex[\"label\"] for ex in examples],\n",
        "    })\n",
        "    ds = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=[\"evidence\", \"claim\"])\n",
        "    return ds\n",
        "\n",
        "import os\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "\n",
        "def train_one_verifier(tag, train_examples, val_examples, out_dir):\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"{tag}\")\n",
        "    print(f\"Output dir: {out_dir}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFIER_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        VERIFIER_MODEL,\n",
        "        num_labels=3,\n",
        "        problem_type=\"single_label_classification\"\n",
        "    )\n",
        "\n",
        "    # class weights from THIS training set\n",
        "    train_labels = [ex[\"label\"] for ex in train_examples]\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.array([0, 1, 2]),\n",
        "        y=train_labels\n",
        "    )\n",
        "    print(f\"{tag} class_weights:\", class_weights)\n",
        "\n",
        "    train_ds = build_hf_dataset(train_examples, tokenizer)\n",
        "    val_ds   = build_hf_dataset(val_examples, tokenizer)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        overwrite_output_dir=False,     # important for resume\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=3,             # keep last 3 checkpoints\n",
        "        learning_rate=1e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=200,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        processing_class=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # Auto-resume if checkpoint exists\n",
        "    last_ckpt = get_last_checkpoint(out_dir)\n",
        "    if last_ckpt is not None:\n",
        "        print(f\"[*] Resuming from checkpoint: {last_ckpt}\")\n",
        "        trainer.train(resume_from_checkpoint=last_ckpt)\n",
        "    else:\n",
        "        print(\"[*] No checkpoint found. Starting fresh training.\")\n",
        "        trainer.train()\n",
        "\n",
        "    model.save_pretrained(out_dir)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "    print(f\"✓ Saved final model to {out_dir}\")\n",
        "\n",
        "    return out_dir\n",
        "\n",
        "\n",
        "# verifier_a_path = train_one_verifier(\n",
        "#     tag=\"Verifier-A (R0 baseline evidence)\",\n",
        "#     train_examples=trainset_a,\n",
        "#     val_examples=valset_a,\n",
        "#     out_dir=os.path.join(EXPERIMENT_DIR, \"verifier_a_r0\")\n",
        "# )\n",
        "\n",
        "verifier_b_path = train_one_verifier(\n",
        "    tag=\"Verifier-B (R1 decomposed evidence)\",\n",
        "    train_examples=trainset_b,\n",
        "    val_examples=valset_b,\n",
        "    out_dir=os.path.join(EXPERIMENT_DIR, \"verifier_b_r1\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655,
          "referenced_widgets": [
            "46276a3f25d64a079cbf275245106606",
            "bed374cb5b694d4f87300d746cc421be",
            "6dc2d821b55444f7af54fbeadd23001d",
            "1175f18b21834b98a84f95805f8f7a79",
            "af3372e831454556bf90e54a6c42fd48",
            "c2e5de05f7b940ceb3f5ff6c2c605787",
            "aab1fa0989bb48cc98cd7d3aeb85de40",
            "e179dcefabdf40ed8016c7d0622ced11",
            "0254d8e93a5e4fd1a4d5c121d7483bcd",
            "46b4ab830fee4c398acced3db6abee1b",
            "535857e962394da696030fca09d8f809",
            "5e75edb7c5f249d1b2fa08b66b856a0f",
            "02ef593206ed49d39c7b7c3383e54d27",
            "17d5ebfd188a44dbad8b74510cfd303a",
            "1553b4a5361e497c98d7d3595d7bd2f5",
            "0601c10e2e1a43d49989ec0909294a13",
            "519ede3581b449eeb6fee859f9e78329",
            "4ee565e372da46b69fd7692f93e2b1c0",
            "3e2b03d94c234d5895a771fa443bf998",
            "54fc86f7b9024c989fdb57b798061289",
            "66f19a8c720246508a81d4d481d7d7ae",
            "f26f8fc792c44d0dbae4eafe47fd3f32"
          ]
        },
        "id": "A83AbZkrqgZ5",
        "outputId": "73f805b4-31a3-4866-ea78-b7124bfd9933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 7: Fine-tuning Verifiers (A=baseline, B=decomposed)\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Verifier-B (R1 decomposed evidence)\n",
            "Output dir: /content/drive/MyDrive/NLP/verifier_b_r1\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifier-B (R1 decomposed evidence) class_weights: [1.80814278 0.57492463 1.41325196]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/29178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46276a3f25d64a079cbf275245106606"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9027 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e75edb7c5f249d1b2fa08b66b856a0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Resuming from checkpoint: /content/drive/MyDrive/NLP/verifier_b_r1/checkpoint-300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Can't find a valid checkpoint at /content/drive/MyDrive/NLP/verifier_b_r1/checkpoint-300",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3570052185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m verifier_b_path = train_one_verifier(\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Verifier-B (R1 decomposed evidence)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mtrain_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3570052185.py\u001b[0m in \u001b[0;36mtrain_one_verifier\u001b[0;34m(tag, train_examples, val_examples, out_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_ckpt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[*] Resuming from checkpoint: {last_ckpt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[*] No checkpoint found. Starting fresh training.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_deepspeed_enabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m             \u001b[0;31m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2299\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINER_STATE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(self, resume_from_checkpoint, model)\u001b[0m\n\u001b[1;32m   2930\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0madapter_subdirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2931\u001b[0m         ):\n\u001b[0;32m-> 2932\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find a valid checkpoint at {resume_from_checkpoint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2934\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model from {resume_from_checkpoint}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find a valid checkpoint at /content/drive/MyDrive/NLP/verifier_b_r1/checkpoint-300"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 8. DOWNSTREAM EVALUATION =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 8: Downstream Evaluation on QuanTemp Test\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load both verifiers\n",
        "print(\"\\nLoading trained verifiers...\")\n",
        "\n",
        "# verifier_a = AutoModelForSequenceClassification.from_pretrained(verifier_a_path).to(device).eval()\n",
        "verifier_b = AutoModelForSequenceClassification.from_pretrained(verifier_b_path).to(device).eval()\n",
        "\n",
        "# You can load either tokenizer path; safest is to load the same base model tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(verifier_a_path)\n",
        "\n",
        "print(\"✓ Verifiers loaded\")\n",
        "\n",
        "# ---- Evidence dicts for each retrieval condition (TEST split) ----\n",
        "evidence_r0 = retrieval_results[\"baseline\"][\"test\"]    # YOUR baseline retrieval\n",
        "evidence_r1 = retrieval_results[\"decomposed\"][\"test\"]  # YOUR decomposition retrieval\n",
        "evidence_r2 = repo_evidence_dict                       # REPO claim-decomp retrieval\n",
        "\n",
        "print(\"\\nEvidence coverage on TEST:\")\n",
        "print(f\"  R0 baseline:   {sum(len(v)>0 for v in evidence_r0.values())}/{len(test_claims)}\")\n",
        "print(f\"  R1 decomposed: {sum(len(v)>0 for v in evidence_r1.values())}/{len(test_claims)}\")\n",
        "print(f\"  R2 repo:       {sum(len(v)>0 for v in evidence_r2.values())}/{len(test_claims)}\")\n",
        "\n",
        "# Evaluation function (top-5, average probs)\n",
        "def evaluate_verifier(model, tokenizer, claims, evidence_dict, retrieval_name, top_n=5):\n",
        "    predictions, true_labels, confidences = [], [], []\n",
        "\n",
        "    for idx, claim_obj in enumerate(tqdm(claims, desc=f\"Evaluating {retrieval_name}\")):\n",
        "        claim_text = claim_obj[\"claim\"]\n",
        "        true_label = normalize_label(claim_obj[\"label\"])\n",
        "\n",
        "        evidences = evidence_dict.get(idx, [])\n",
        "        if not evidences:\n",
        "            predictions.append(2)   # NEI\n",
        "            true_labels.append(true_label)\n",
        "            confidences.append(1.0)\n",
        "            continue\n",
        "\n",
        "        all_probs = []\n",
        "        for ev in evidences[:top_n]:\n",
        "            ev = ev.strip()\n",
        "            if len(ev) < 20:\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                ev[:512],\n",
        "                claim_text,\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "                probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "\n",
        "        if not all_probs:\n",
        "            predictions.append(2)   # NEI\n",
        "            true_labels.append(true_label)\n",
        "            confidences.append(1.0)\n",
        "            continue\n",
        "\n",
        "        avg_probs = np.mean(all_probs, axis=0)\n",
        "        pred = int(np.argmax(avg_probs))\n",
        "        conf = float(avg_probs.max())\n",
        "\n",
        "        predictions.append(pred)\n",
        "        true_labels.append(true_label)\n",
        "        confidences.append(conf)\n",
        "\n",
        "    macro_f1 = f1_score(true_labels, predictions, average=\"macro\", zero_division=0)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    per_class_f1 = f1_score(true_labels, predictions, average=None, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"retrieval\": retrieval_name,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"supports_f1\": per_class_f1[0],\n",
        "        \"refutes_f1\": per_class_f1[1],\n",
        "        \"nei_f1\": per_class_f1[2],\n",
        "        \"predictions\": predictions,\n",
        "        \"true_labels\": true_labels,\n",
        "        \"confidences\": confidences,\n",
        "    }\n",
        "\n",
        "results_matrix = []\n",
        "\n",
        "def run_combo(model, model_name, evidence_dict, retrieval_name):\n",
        "    res = evaluate_verifier(model, tokenizer, test_claims, evidence_dict, retrieval_name=retrieval_name, top_n=5)\n",
        "    res[\"verifier\"] = model_name\n",
        "    results_matrix.append(res)\n",
        "    print(f\"{model_name} × {retrieval_name}: Macro-F1={res['macro_f1']:.4f}, Acc={res['accuracy']:.4f}\")\n",
        "    return res\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Evaluating Verifier-A (trained on R0 evidence)\")\n",
        "print(\"-\"*80)\n",
        "run_combo(verifier_a, \"Verifier-A (R0-trained)\", evidence_r0, \"R0_Baseline\")\n",
        "run_combo(verifier_a, \"Verifier-A (R0-trained)\", evidence_r1, f\"R1_{BEST_DECOMP_KEY}\")\n",
        "run_combo(verifier_a, \"Verifier-A (R0-trained)\", evidence_r2, \"R2_RepoClaimDecomp\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Evaluating Verifier-B (trained on R1 evidence)\")\n",
        "print(\"-\"*80)\n",
        "run_combo(verifier_b, \"Verifier-B (R1-trained)\", evidence_r0, \"R0_Baseline\")\n",
        "run_combo(verifier_b, \"Verifier-B (R1-trained)\", evidence_r1, f\"R1_{BEST_DECOMP_KEY}\")\n",
        "run_combo(verifier_b, \"Verifier-B (R1-trained)\", evidence_r2, \"R2_RepoClaimDecomp\")\n"
      ],
      "metadata": {
        "id": "yS6ONFPtr6c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 9. SAVE RESULTS =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 9: Saving Results & Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create summary table\n",
        "summary_rows = []\n",
        "for result in results_matrix:\n",
        "    summary_rows.append({\n",
        "        'verifier': result['verifier'],\n",
        "        'retrieval': result['retrieval'],\n",
        "        'accuracy': result['accuracy'],\n",
        "        'macro_f1': result['macro_f1'],\n",
        "        'supports_f1': result['supports_f1'],\n",
        "        'refutes_f1': result['refutes_f1'],\n",
        "        'nei_f1': result['nei_f1']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_path = os.path.join(EXPERIMENT_DIR, \"downstream_results_summary.csv\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "print(f\"\\n✓ Saved summary to {summary_path}\")\n",
        "print(\"\\nDOWNSTREAM RESULTS SUMMARY:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Find best configuration\n",
        "best_config = summary_df.loc[summary_df['macro_f1'].idxmax()]\n",
        "print(f\"\\n🏆 BEST CONFIGURATION:\")\n",
        "print(f\"   {best_config['verifier']} × {best_config['retrieval']}\")\n",
        "print(f\"   Macro-F1: {best_config['macro_f1']:.4f} ({best_config['macro_f1']*100:.2f}%)\")\n",
        "print(f\"   Accuracy: {best_config['accuracy']:.4f} ({best_config['accuracy']*100:.2f}%)\")\n",
        "\n",
        "# Save detailed predictions for each configuration\n",
        "for result in results_matrix:\n",
        "    pred_rows = []\n",
        "    for i, (claim, true, pred, conf) in enumerate(zip(\n",
        "        result['claims'],\n",
        "        result['true_labels'],\n",
        "        result['predictions'],\n",
        "        result['confidences']\n",
        "    )):\n",
        "        pred_rows.append({\n",
        "            'claim_id': i,\n",
        "            'claim': claim,\n",
        "            'true_label': true,\n",
        "            'pred_label': pred,\n",
        "            'true_str': ['SUPPORTS', 'REFUTES', 'NEI'][true],\n",
        "            'pred_str': ['SUPPORTS', 'REFUTES', 'NEI'][pred],\n",
        "            'confidence': conf,\n",
        "            'correct': true == pred\n",
        "        })\n",
        "\n",
        "    pred_df = pd.DataFrame(pred_rows)\n",
        "    pred_path = os.path.join(\n",
        "        EXPERIMENT_DIR,\n",
        "        f\"predictions_{result['verifier'].replace(' ', '_').replace('(', '').replace(')', '')}_{result['retrieval']}.csv\"\n",
        "    )\n",
        "    pred_df.to_csv(pred_path, index=False)\n",
        "    print(f\"   Saved predictions: {os.path.basename(pred_path)}\")\n",
        "\n",
        "# Generate qualitative examples\n",
        "print(\"\\nGenerating qualitative examples...\")\n",
        "\n",
        "qualitative_path = os.path.join(EXPERIMENT_DIR, \"qualitative_examples.txt\")\n",
        "\n",
        "with open(qualitative_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"QUALITATIVE ANALYSIS: Decomposition Impact\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"Best Decomposition Method: {DECOMPOSITION_METHODS[BEST_DECOMP_KEY]['name']}\\n\")\n",
        "    f.write(f\"FactIR MRR: {best_method['mrr']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"DOWNSTREAM PERFORMANCE COMPARISON\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    # Compare baseline vs decomposition\n",
        "    baseline_f1 = summary_df[summary_df['retrieval'] == 'R0_Baseline']['macro_f1'].values[0]\n",
        "    decomp_f1 = summary_df[summary_df['retrieval'].str.contains('R1')]['macro_f1'].max()\n",
        "    repo_f1 = summary_df[summary_df['retrieval'] == 'R2_RepoClaimDecomp']['macro_f1'].values[0]\n",
        "\n",
        "    f.write(f\"Baseline (R0):         Macro-F1 = {baseline_f1:.4f}\\n\")\n",
        "    f.write(f\"Our Decomp (R1):       Macro-F1 = {decomp_f1:.4f}\\n\")\n",
        "    f.write(f\"Repo Decomp (R2):      Macro-F1 = {repo_f1:.4f}\\n\\n\")\n",
        "\n",
        "    if decomp_f1 > baseline_f1:\n",
        "        improvement = ((decomp_f1 - baseline_f1) / baseline_f1) * 100\n",
        "        f.write(f\"✓ Our decomposition IMPROVED performance by {improvement:.2f}%\\n\\n\")\n",
        "    else:\n",
        "        f.write(f\"✗ Our decomposition did not improve over baseline\\n\\n\")\n",
        "\n",
        "    if decomp_f1 > repo_f1:\n",
        "        f.write(f\"✓ Our decomposition OUTPERFORMED repo decomposition\\n\\n\")\n",
        "    else:\n",
        "        f.write(f\"✗ Repo decomposition performed better\\n\\n\")\n",
        "\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"KEY FINDINGS\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"1. Best FactIR decomposition: {BEST_DECOMP_KEY}\\n\")\n",
        "    f.write(f\"   - Achieved MRR of {best_method['mrr']:.4f}\\n\")\n",
        "    f.write(f\"   - Hit@5: {best_method['hit@5']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(f\"2. Best downstream configuration:\\n\")\n",
        "    f.write(f\"   - {best_config['verifier']} × {best_config['retrieval']}\\n\")\n",
        "    f.write(f\"   - Macro-F1: {best_config['macro_f1']:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(f\"3. Training approach impact:\\n\")\n",
        "    verifier_a_best = summary_df[summary_df['verifier'].str.contains('A')]['macro_f1'].max()\n",
        "    verifier_b_best = summary_df[summary_df['verifier'].str.contains('B')]['macro_f1'].max()\n",
        "    if verifier_b_best > verifier_a_best:\n",
        "        f.write(f\"   ✓ Decomp-trained verifier performed better ({verifier_b_best:.4f} vs {verifier_a_best:.4f})\\n\\n\")\n",
        "    else:\n",
        "        f.write(f\"   - Baseline-trained verifier performed better ({verifier_a_best:.4f} vs {verifier_b_best:.4f})\\n\\n\")\n",
        "\n",
        "print(f\"✓ Saved qualitative analysis to {qualitative_path}\")\n",
        "\n",
        "# ===== 10. FINAL SUMMARY =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n📁 All results saved to: {EXPERIMENT_DIR}\")\n",
        "print(\"\\nGenerated files:\")\n",
        "print(f\"  1. factir_ranking_results.csv\")\n",
        "print(f\"  2. downstream_results_summary.csv\")\n",
        "print(f\"  3. training_pair_stats.json\")\n",
        "print(f\"  4. qualitative_examples.txt\")\n",
        "print(f\"  5. Prediction CSVs for each configuration\")\n",
        "print(f\"  6. Trained verifier checkpoints\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. Provide evidence corpus JSON for true retrieval experiments\")\n",
        "print(\"2. Update CORPUS_PATH variable to enable custom BM25 retrieval\")\n",
        "print(\"3. Review qualitative_examples.txt for insights\")\n",
        "print(\"4. Compare results with your friend's baseline\")\n",
        "print(\"5. Experiment with other decomposition prompts if needed\")\n",
        "\n",
        "print(\"\\n✓ Pipeline execution complete!\")\n",
        "print(\"\\nTo download results:\")\n",
        "print(f\"  from google.colab import files\")\n",
        "print(f\"  files.download('{summary_path}')\")\n",
        "print(f\"  files.download('{qualitative_path}')\")"
      ],
      "metadata": {
        "id": "6TQj3znWsHOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}